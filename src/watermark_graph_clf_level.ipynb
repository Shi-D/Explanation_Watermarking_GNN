{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   embed_and_verify import *\n",
    "from   config import *\n",
    "from   prune_and_fine_tune_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "\n",
    "from   scipy import stats\n",
    "from   scipy.stats import anderson, kstest, zscore, shapiro\n",
    "\n",
    "import torch\n",
    "from   torch.nn import ModuleList\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from   torch_geometric.datasets import Reddit2\n",
    "from   torch_geometric.nn import GATConv, GCNConv, GraphConv, SAGEConv, GINConv, global_mean_pool\n",
    "from   torch_geometric.transforms import RandomLinkSplit\n",
    "from   torch_geometric.data import Data\n",
    "from   torch_geometric.loader import DataLoader\n",
    "\n",
    "import zipfile\n",
    "\n",
    "from   pcgrad.pcgrad import PCGrad \n",
    "\n",
    "mpl.rcParams['figure.dpi']=250\n",
    "\n",
    "root_folder = '<path_to_repo_directory>'\n",
    "training_folder = os.path.join(root_folder,'training_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64547f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def download_file(url, filename):\n",
    "    response = requests.get(url)\n",
    "    with open(f'../data/{filename}', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    dataset_name = filename.replace('.zip','')\n",
    "    \n",
    "\n",
    "    unzip_file(f'../data/{filename}', f'../data',)\n",
    "    os.rename(f'../data/{dataset_name}',f'../data/{dataset_name}_raw',)\n",
    "    print(f\"Downloaded {dataset_name}\")\n",
    "\n",
    "\n",
    "def unzip_file(zip_path, extract_to=\".\"):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(f\"Extracted {zip_path} to {extract_to}\")\n",
    "\n",
    "\n",
    "def create_dataset_from_files(root, dataset_name):\n",
    "    \"\"\"\n",
    "    Converts raw TU Dataset format files into PyTorch Geometric Data objects.\n",
    "    \n",
    "    Args:\n",
    "        root (str): Path to the directory containing raw dataset files\n",
    "        dataset_name (str): Name of the dataset (used to construct filenames)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of PyTorch Geometric Data objects, one per graph\n",
    "    \n",
    "    Process:\n",
    "        1. Reads four core files:\n",
    "           - {dataset_name}_A.txt: Edge list (node pairs)\n",
    "           - {dataset_name}_graph_indicator.txt: Which graph each node belongs to\n",
    "           - {dataset_name}_graph_labels.txt: Graph-level labels for classification\n",
    "           - {dataset_name}_node_labels.txt: Node-level features/labels\n",
    "        2. For each graph in the dataset:\n",
    "           - Identifies nodes belonging to that graph\n",
    "           - Maps global node indices to local indices\n",
    "           - Extracts edges within the graph\n",
    "           - Converts node labels to one-hot encoded features\n",
    "           - Creates a Data object with x (features), edge_index, and y (label)\n",
    "        3. Transforms binary labels from [-1,1] to [0,1] format\n",
    "    \n",
    "    Note: Assumes TU Dataset standard format with 1-indexed graphs\n",
    "    \"\"\"\n",
    "    # Load raw files\n",
    "    edges = pd.read_csv(os.path.join(root, f\"{dataset_name}_A.txt\"), header=None, sep=\",\")\n",
    "    graph_indicator = pd.read_csv(os.path.join(root, f\"{dataset_name}_graph_indicator.txt\"), header=None)\n",
    "    graph_labels = pd.read_csv(os.path.join(root, f\"{dataset_name}_graph_labels.txt\"), header=None)\n",
    "    node_labels = pd.read_csv(os.path.join(root, f\"{dataset_name}_node_labels.txt\"), header=None)\n",
    "\n",
    "    data_list = []\n",
    "    N = graph_labels.shape[0]  # Number of graphs in the dataset\n",
    "    indices_to_use = range(1, N + 1)  # Graph indices start from 1\n",
    "\n",
    "\n",
    "    # Process each graph\n",
    "    for c, i in enumerate(indices_to_use):\n",
    "        \n",
    "        # Find nodes belonging to the current graph\n",
    "        node_indices = graph_indicator[graph_indicator[0] == i].index\n",
    "\n",
    "        # Map global node indices to local indices\n",
    "        node_idx_map = {idx: j for j, idx in enumerate(node_indices)}\n",
    "        graph_edges = edges[edges[0].isin(node_indices + 1) & edges[1].isin(node_indices + 1)]\n",
    "        # graph_edges = graph_edges.applymap(lambda x: node_idx_map[x - 1])\n",
    "        graph_edges = graph_edges.apply(lambda col: col.map(lambda x: node_idx_map[x - 1]))\n",
    "        edge_index = torch.tensor(graph_edges.values, dtype=torch.long).t().contiguous()\n",
    "\n",
    "        x = torch.tensor(node_labels.iloc[node_indices].values).squeeze()\n",
    "        num_classes = node_labels[0].max() + 1 \n",
    "        x = F.one_hot(x.clone().detach().long(), num_classes=num_classes).float() \n",
    "        y = torch.tensor(graph_labels.iloc[i - 1].values, dtype=torch.long)\n",
    "        y = (y + 1) // 2  # Transform [-1, 1] -> [0, 1]\n",
    "        # Create a Data object for the current graph\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        data.original_index = c\n",
    "        data_list.append(data)\n",
    "\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def split_dataset(data_list, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
    "    \"\"\"\n",
    "    Splits a list of graphs into train/validation/test sets for graph classification.\n",
    "    \n",
    "    Args:\n",
    "        data_list (list): List of PyTorch Geometric Data objects\n",
    "        train_ratio (float): Fraction of data for training (default: 0.7)\n",
    "        val_ratio (float): Fraction of data for validation (default: 0.15)\n",
    "        test_ratio (float): Fraction of data for testing (default: 0.15)\n",
    "        seed (int): Random seed for reproducible splits\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_data, val_data, test_data) as lists of Data objects\n",
    "\n",
    "    Note: Operates on entire graphs, not individual nodes\n",
    "    \"\"\"\n",
    "    # Ensure ratios sum to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1\"\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Shuffle indices\n",
    "    indices = list(range(len(data_list)))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    # Split indices\n",
    "    train_end = int(train_ratio * len(indices))\n",
    "    val_end = train_end + int(val_ratio * len(indices))\n",
    "    \n",
    "    train_indices = indices[:train_end]\n",
    "    val_indices = indices[train_end:val_end]\n",
    "    test_indices = indices[val_end:]\n",
    "\n",
    "    # Create subsets\n",
    "    train_data = [data_list[i] for i in train_indices]\n",
    "    val_data = [data_list[i] for i in val_indices]\n",
    "    test_data = [data_list[i] for i in test_indices]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "def generate_subgraph_graph_clf(data, subgraph_size=5,seed=0):\n",
    "    \"\"\"\n",
    "    Extracts a random subgraph from a single graph for watermarking purposes.\n",
    "    \n",
    "    Args:\n",
    "        data (Data): PyTorch Geometric Data object representing a graph\n",
    "        subgraph_size (int): Number of nodes to include in the subgraph\n",
    "        seed (int): Random seed for reproducible subgraph selection\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (data_sub, subgraph_signature, subgraph_node_idx)\n",
    "            - data_sub: Data object containing the extracted subgraph\n",
    "            - subgraph_signature: String identifier for this specific subgraph\n",
    "            - subgraph_node_idx: Tensor of original node indices in the subgraph\n",
    "    \n",
    "    Note: Preserves the original graph's classification label for the subgraph\n",
    "    \"\"\"\n",
    "    data = data.clone()\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    num_nodes = data.x.shape[0]\n",
    "    subgraph_node_idx = torch.tensor(random.sample(list(range(num_nodes)), min(subgraph_size, num_nodes)))\n",
    "    sub_edge_index, _ = subgraph(subgraph_node_idx, data.edge_index, relabel_nodes=True, num_nodes=num_nodes)\n",
    "    data_sub = Data(\n",
    "        x=data.x[subgraph_node_idx] if data.x is not None else None,\n",
    "        edge_index=sub_edge_index,\n",
    "        y=data.y)\n",
    "    del sub_edge_index\n",
    "    subgraph_signature = '_'.join([str(s) for s in subgraph_node_idx.tolist()])\n",
    "    subgraph_signature = str(data.original_index) + '_' + subgraph_signature\n",
    "    return data_sub, subgraph_signature, subgraph_node_idx\n",
    "\n",
    "\n",
    "\n",
    "def build_subgraph_collections(train_data, num_collections, num_subgraphs_per_collection, subgraph_size, proportion_features_to_watermark, aggregate_method='flatten',seed=0):\n",
    "    \"\"\"\n",
    "    Creates collections of subgraphs from multiple training graphs for coordinated watermarking.\n",
    "    \n",
    "    Args:\n",
    "        train_data (list): List of training Data objects\n",
    "        num_collections (int): Number of watermark collections to create\n",
    "        num_subgraphs_per_collection (int): How many subgraphs per collection\n",
    "        subgraph_size (int): Number of nodes in each subgraph\n",
    "        proportion_features_to_watermark (float): Fraction of features to watermark (0.0-1.0)\n",
    "        aggregate_method (str): How to combine node features ('flatten', 'average', 'sum')\n",
    "        seed (int): Random seed for reproducible collection building\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (subgraph_collection_dict, most_represented_indices, graphs_used)\n",
    "            - subgraph_collection_dict: Nested dict with collections, subgraphs, features, watermarks\n",
    "            - most_represented_indices: Indices of most frequently non-zero features\n",
    "            - graphs_used: List of original graph indices used in collections\n",
    "    \n",
    "    Note: Returns list of used graphs so they can be excluded from training\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    assert aggregate_method in ['flatten','average','sum']\n",
    "    subgraph_collection_dict = {i:{'subgraphs':{},'feature_matrix':None} for i in range(num_collections)}\n",
    "    graphs_used = []\n",
    "    for i in range(num_collections):\n",
    "        graph_indices = random.sample(list(range(len(train_data))),num_subgraphs_per_collection)\n",
    "        graph_indices_original = [train_data[i].original_index for i in graph_indices]\n",
    "        graphs_used.extend(graph_indices_original)\n",
    "        graph_features = []\n",
    "        for idx in graph_indices:\n",
    "            graph = train_data[idx]\n",
    "            this_subgraph, subgraph_signature, _ =  generate_subgraph_graph_clf(graph, subgraph_size=subgraph_size,seed=seed)\n",
    "            subgraph_collection_dict[i]['subgraphs'][subgraph_signature]=this_subgraph\n",
    "            if aggregate_method=='flatten':\n",
    "                graph_features.append(this_subgraph.x.flatten().tolist()) # average node features\n",
    "            elif aggregate_method=='average':\n",
    "                graph_features.append(torch.mean(this_subgraph.x,dim=0).tolist()) # average node features\n",
    "            elif aggregate_method=='sum':\n",
    "                graph_features.append(torch.sum(this_subgraph.x,dim=0).tolist()) # average node features\n",
    "        graph_features = torch.tensor(graph_features)\n",
    "        subgraph_collection_dict[i]['feature_matrix']=graph_features\n",
    "\n",
    "\n",
    "\n",
    "    subgraph_collection_features_concat = torch.vstack([subgraph_collection_dict[k]['feature_matrix'] for k in subgraph_collection_dict.keys()])\n",
    "    nonzero_feat_mask = subgraph_collection_features_concat!=0\n",
    "    nonzero_feat_counts = torch.sum(nonzero_feat_mask,dim=0)\n",
    "    sorted_indices = torch.argsort(nonzero_feat_counts, descending=True)\n",
    "    len_watermark = int(np.floor(proportion_features_to_watermark*len(sorted_indices)))\n",
    "    most_represented_indices = sorted_indices[:len_watermark]\n",
    "\n",
    "\n",
    "    watermarks = create_watermarks_at_most_represented_indices(num_collections, len_watermark, subgraph_collection_features_concat.shape[1], most_represented_indices, seed)\n",
    "    for (k,wmk) in zip(subgraph_collection_dict.keys(),watermarks):\n",
    "        subgraph_collection_dict[k]['watermark']=wmk\n",
    "    return subgraph_collection_dict, most_represented_indices, list(set(graphs_used))\n",
    "\n",
    "\n",
    "def regress_on_subgraph_collections(model, subgraph_collection_dict, collection_id, mode='train'):\n",
    "    collection = subgraph_collection_dict[collection_id]\n",
    "    betas = []\n",
    "    for subgraph_signature in collection.keys():\n",
    "        subgraph = collection['subgraphs'][subgraph_signature]\n",
    "        x_sub = collection['feature_matrix']\n",
    "        y_sub = model(subgraph, mode)\n",
    "        beta = solve_regression(x_sub,y_sub)\n",
    "        betas.append(beta)\n",
    "    return betas\n",
    "\n",
    "class graph_clf_model(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, num_layers, hidden_channels, dropout, conv_fn):\n",
    "        super(graph_clf_model, self).__init__()\n",
    "        self.layers = ModuleList([conv_fn(num_node_features, hidden_channels)])\n",
    "        self.layers += [conv_fn(hidden_channels,hidden_channels)]*(num_layers-2)\n",
    "        self.layers += [conv_fn(hidden_channels,num_classes)]\n",
    "        self.dropout= dropout\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = torch.nn.Dropout(p=self.dropout)(x)\n",
    "\n",
    "        x = self.layers[-1](x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def collect_random_subgraphs_graph_clf(dataset, subgraph_size, num_graphs):\n",
    "    \"\"\"\n",
    "    Generates random subgraphs for null distribution creation in significance testing.\n",
    "    \n",
    "    Args:\n",
    "        dataset (list): Complete dataset of graphs\n",
    "        subgraph_size (int): Number of nodes per subgraph\n",
    "        num_graphs (int): How many random subgraphs to generate\n",
    "    \n",
    "    Returns:\n",
    "        list: List of Data objects representing random subgraphs\n",
    "    \"\"\"\n",
    "    subgraph_list = []\n",
    "    for _ in range(num_graphs):\n",
    "        graph_index = random.choice(range(len(dataset)))\n",
    "        data = dataset[graph_index]\n",
    "        num_nodes = data.x.shape[0]\n",
    "        subgraph_node_idx = torch.tensor(random.sample(list(range(num_nodes)), min(subgraph_size, num_nodes)))\n",
    "        sub_edge_index, _ = subgraph(subgraph_node_idx, data.edge_index, relabel_nodes=True, num_nodes=num_nodes)\n",
    "        data_sub = Data(\n",
    "            x=data.x[subgraph_node_idx] if data.x is not None else None,\n",
    "            edge_index=sub_edge_index,\n",
    "            y=data.y)\n",
    "        subgraph_list.append(data_sub)\n",
    "    return subgraph_list\n",
    "\n",
    "\n",
    "\n",
    "def train_with_watermark(dataset_name,\n",
    "                         num_node_features,\n",
    "                         num_classes,\n",
    "                         train_ratio,\n",
    "                         val_ratio,\n",
    "                         test_ratio,\n",
    "                         num_layers = 4,\n",
    "                         lr  = 0.001,\n",
    "                         epochs  = 4000,\n",
    "                         hidden_channels  = 64,\n",
    "                         dropout  = 0,\n",
    "                         proportion_features_to_watermark = 1,\n",
    "                         num_subgraphs_per_collection = 5,\n",
    "                         subgraph_size = 10,\n",
    "                         num_collections = 5,\n",
    "                         epsilon = 0.01,\n",
    "                         regression_lambda = 0.01,\n",
    "                         coefWmk  = 80,\n",
    "                         batch_size = 10,\n",
    "                         use_pcgrad  = True,\n",
    "                         conv_fn  = GraphConv,\n",
    "                         aggregate_method = 'average',\n",
    "                         num_iter = 1000,\n",
    "                         seed = 0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Complete training pipeline for graph classification with integrated watermarking.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load\n",
    "        num_node_features (int): Dimension of node feature vectors\n",
    "        num_classes (int): Number of classes for graph classification\n",
    "        train_ratio, val_ratio, test_ratio (float): Data split ratios\n",
    "        **kwargs: Additional hyperparameters including:\n",
    "            - num_layers, lr, epochs, hidden_channels, dropout: Model architecture\n",
    "            - num_collections, num_subgraphs_per_collection, subgraph_size: Watermarking\n",
    "            - epsilon, coefWmk: Watermark loss parameters\n",
    "            - use_pcgrad: Whether to use PCGrad optimizer\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, subgraph_collection_dict, primary_loss_curve, watermark_loss_curve, mu, sig, p)\n",
    "            - model: Trained graph classification model\n",
    "            - subgraph_collection_dict: Watermark collections used\n",
    "            - primary/watermark_loss_curve: Training loss histories\n",
    "            - mu, sig: Natural distribution parameters for significance testing\n",
    "            - p: P-value for watermark significance\n",
    "    \n",
    "    Process:\n",
    "        1. Dataset Loading: Creates dataset from raw files and splits into train/val/test\n",
    "        2. Model Setup: Initializes graph classification model with specified architecture\n",
    "        3. Watermark Preparation: Builds subgraph collections and removes used graphs from training\n",
    "        4. Training Loop: For each epoch:\n",
    "           - Forward pass on training data (classification loss)\n",
    "           - Forward pass on watermark collections (watermark loss)\n",
    "           - Regression analysis on watermark predictions\n",
    "           - Combined loss backpropagation\n",
    "           - Validation accuracy computation\n",
    "        5. Testing: Final model evaluation on test set\n",
    "        6. Statistical Validation: \n",
    "           - Generates random subgraph baseline (null distribution)\n",
    "           - Computes natural distribution parameters (mu, sigma)\n",
    "           - Calculates p-value for watermark significance\n",
    "        7. Persistence: Saves model, results, and statistical data\n",
    "    \n",
    "    Key Features:\n",
    "        - Dual-loss optimization (classification + watermark)\n",
    "        - Real-time watermark alignment monitoring\n",
    "        - Statistical significance testing\n",
    "        - Automatic result organization\n",
    "    \"\"\"\n",
    "    \n",
    "    if aggregate_method=='flatten':\n",
    "        get_x = lambda sub: sub.x.flatten().tolist()\n",
    "    elif aggregate_method=='average':\n",
    "        get_x = lambda sub: torch.mean(sub.x, dim=0).tolist()\n",
    "    elif aggregate_method=='sum':\n",
    "        get_x = lambda sub: torch.sum(sub.x, dim=0).tolist()\n",
    "\n",
    "    data_list = create_dataset_from_files(f\"../data/{dataset_name}_raw\", dataset_name)\n",
    "    train_data, val_data, test_data = split_dataset(data_list, train_ratio=train_ratio, val_ratio=val_ratio, test_ratio=test_ratio)\n",
    "    val_loader = DataLoader(train_data, batch_size=500, shuffle=True)\n",
    "    test_loader = DataLoader(train_data, batch_size=500, shuffle=True)\n",
    "    model =  graph_clf_model(num_node_features, num_classes, num_layers, hidden_channels, dropout, conv_fn)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    if use_pcgrad==True:\n",
    "        optimizer = PCGrad(optimizer)\n",
    "\n",
    "    subgraph_collection_dict, most_represented_indices, graphs_used = build_subgraph_collections(train_data, num_collections, num_subgraphs_per_collection, subgraph_size, proportion_features_to_watermark, aggregate_method, seed)\n",
    "    train_data = [d for d in train_data if d.original_index not in graphs_used]\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    primary_loss_curve, watermark_loss_curve = [],[]\n",
    "    observed_matches = []\n",
    "    for e in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_primary, loss_watermark = torch.tensor(0.0), torch.tensor(0.0)\n",
    "        train_accuracies, val_accuracies = torch.tensor([]), torch.tensor([])\n",
    "        raw_betas = []\n",
    "        watermark_alignment_rates = []\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            log_logits_train  = model(batch.x, batch.edge_index, batch=batch.batch)\n",
    "            loss_primary  += F.nll_loss(log_logits_train, batch.y)\n",
    "            train_accuracies = torch.cat((train_accuracies, accuracy(log_logits_train, batch.y).unsqueeze(dim=0)), dim=0)\n",
    "\n",
    "        for k in subgraph_collection_dict.keys():\n",
    "            this_collection = subgraph_collection_dict[k]\n",
    "            this_watermark = this_collection['watermark']\n",
    "            x_sub = this_collection['feature_matrix']\n",
    "            y_sub = []\n",
    "            for subgraph_ in this_collection['subgraphs'].values():\n",
    "                y_sub.append(model(subgraph_.x, subgraph_.edge_index))\n",
    "            y_sub = torch.vstack(y_sub)\n",
    "            omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False)\n",
    "            this_raw_beta = solve_regression(x_sub, y_sub, regression_lambda)\n",
    "            raw_betas.append(this_raw_beta)\n",
    "            beta          = process_beta(this_raw_beta, omit_indices)\n",
    "            B_x_W = (beta*this_watermark).clone()\n",
    "            B_x_W = B_x_W[not_omit_indices]\n",
    "            sign_beta_most_rep = torch.sign(beta)[most_represented_indices]\n",
    "            wmk_most_rep = this_watermark[most_represented_indices]\n",
    "            watermark_alignment_rates.append(torch.sum(torch.eq(sign_beta_most_rep, wmk_most_rep)).item()/len(most_represented_indices))\n",
    "            beta_weights = torch.ones(len(not_omit_indices))\n",
    "            loss_watermark += torch.mean(torch.clamp(epsilon-B_x_W, min=0)*beta_weights)\n",
    "\n",
    "        model.eval()\n",
    "        for batch in val_loader:\n",
    "            log_logits_val  = model(batch.x, batch.edge_index, batch=batch.batch)\n",
    "            val_accuracies = torch.cat((val_accuracies, accuracy(log_logits_val, batch.y).unsqueeze(dim=0)), dim=0)\n",
    "\n",
    "        stacked_sign_betas= torch.sign(torch.vstack(raw_betas))\n",
    "        match_count_with_zeros = count_matches(stacked_sign_betas, ignore_zeros=False)\n",
    "        match_count_without_zeros = count_matches(stacked_sign_betas, ignore_zeros=True)\n",
    "\n",
    "        observed_matches.append(match_count_without_zeros)\n",
    "\n",
    "        acc_trn = torch.mean(train_accuracies).item()\n",
    "        acc_val = torch.mean(val_accuracies).item()\n",
    "        avg_wmk_alignment = np.mean(watermark_alignment_rates)\n",
    "        loss = loss_primary + coefWmk*loss_watermark\n",
    "        primary_loss_curve.append(loss_primary.item())\n",
    "        watermark_loss_curve.append(coefWmk*loss_watermark.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if e%10==0:\n",
    "            epoch_printout = f'Epoch: {e:3d}, L (clf/wmk) = {loss_primary:.3f}/{coefWmk*loss_watermark:.3f}, acc (trn/val)= {acc_trn:.3f}/{acc_val:.3f}, #_match_WMK w/wout 0s = {match_count_with_zeros}/{match_count_without_zeros}, alignment={avg_wmk_alignment:.3f}'\n",
    "            print(epoch_printout)\n",
    "    test_accuracies = torch.tensor([])\n",
    "    for batch in test_loader:\n",
    "        log_logits_test  = model(batch.x, batch.edge_index, batch=batch.batch)\n",
    "        test_accuracies = torch.cat((test_accuracies, accuracy(log_logits_test, batch.y).unsqueeze(dim=0)), dim=0)\n",
    "    epoch_printout = f'Epoch: {e:3d}, L (clf/wmk) = {loss_primary:.3f}/{coefWmk*loss_watermark:.3f}, acc (trn/val)= {acc_trn:.3f}/{acc_val:.3f}, #_match_WMK w/wout 0s = {match_count_with_zeros}/{match_count_without_zeros}, alignment={avg_wmk_alignment:.3f}'\n",
    "    print(epoch_printout)\n",
    "    print(f'Test accuracy: {torch.mean(test_accuracies).item():.3f}')\n",
    "\n",
    "    final_observed_match_count = np.mean(observed_matches[-5:])\n",
    "    data_list = create_dataset_from_files(f\"../data/{dataset_name}_raw\", dataset_name)\n",
    "    random_subgraph_list = collect_random_subgraphs_graph_clf(data_list, subgraph_size, 200)\n",
    "    all_match_counts = []\n",
    "    for i in range(num_iter):\n",
    "        print(f'{i}/{num_iter}',end='\\r')\n",
    "        betas = []\n",
    "        for c in range(num_collections):\n",
    "            subgraph_choices = random.sample(range(len(random_subgraph_list)),num_subgraphs_per_collection)\n",
    "            subgraphs_ = [random_subgraph_list[idx] for idx in subgraph_choices]\n",
    "            y_subs = torch.vstack([model(s.x, s.edge_index) for s in subgraphs_])\n",
    "            x_subs = torch.tensor([get_x(s) for s in subgraphs_])\n",
    "            this_raw_beta = solve_regression(x_subs, y_subs, regression_lambda)\n",
    "            betas.append(torch.sign(this_raw_beta))\n",
    "        betas = torch.vstack(betas)\n",
    "        all_match_counts.append(count_matches(betas,ignore_zeros=True))\n",
    "    mu = np.mean(all_match_counts)\n",
    "    sig = np.std(all_match_counts)\n",
    "    print('observed_matches[-5:]:',observed_matches[-5:])\n",
    "    print('average final observed match:',final_observed_match_count)\n",
    "    print(f'Naturally-occurring match counts (mu,sig): ({mu},{sig})')\n",
    "    z = (final_observed_match_count-mu)/sig\n",
    "    p = scipy.stats.norm.sf(z)\n",
    "    print('p_value:',p)\n",
    "    \n",
    "\n",
    "    root_ = f'../{dataset_name}_results/{num_collections}_collections_{num_subgraphs_per_collection}_subgraphs_size_{subgraph_size}_coefWmk{coefWmk}_numLayers{num_layers}_hiddenDim{hidden_channels}_epsilon{epsilon}_dropout{dropout}_lr{lr}'\n",
    "    try:\n",
    "        os.mkdir(root_)\n",
    "    except:\n",
    "        pass\n",
    "    root_ += f'/seed{seed}'\n",
    "    try:\n",
    "        os.mkdir(root_)\n",
    "    except:\n",
    "        pass\n",
    "    with open(os.path.join(root_,'all_match_counts.pkl'),'wb') as f:\n",
    "        pickle.dump(all_match_counts,f)\n",
    "    with open(os.path.join(root_,f'model.pkl'),'wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "    with open(os.path.join(root_,f'acc_trn.pkl'),'wb') as f:\n",
    "        pickle.dump(acc_trn,f)\n",
    "    with open(os.path.join(root_,f'acc_val.pkl'),'wb') as f:\n",
    "        pickle.dump(acc_val,f)\n",
    "    return model, subgraph_collection_dict, primary_loss_curve, watermark_loss_curve, mu, sig, p\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae9d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: need to download the dataset. You can get many dataset here: https://chrsmrrs.github.io/datasets/docs/datasets/\n",
    "# You will need to format the dataset into the correct format: see below for an example for how it needs to be.\n",
    "\n",
    "\n",
    "dataset_name='MUTAG'\n",
    "\n",
    "\n",
    "url = f\"https://www.chrsmrrs.com/graphkerneldatasets/{dataset_name}.zip\"\n",
    "download_file(url, f\"{dataset_name}.zip\")\n",
    "\n",
    "root = f\"../data/{dataset_name}_raw\"  # Path to raw files\n",
    "\n",
    "num_collections = 5\n",
    "num_subgraphs_per_collection = 5\n",
    "subgraph_size = 10\n",
    "train_ratio, test_ratio, val_ratio = 0.7, 0.15, 0.15\n",
    "num_node_features = 7 #You may need to look this up depending on the dataset you're using. MUTAG has 7 node features.\n",
    "num_classes=2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for subgraph_size in [10,20]:\n",
    "    for num_collections in [2,3,4,5,6]:\n",
    "        for seed in range(5):\n",
    "            random.seed(seed)\n",
    "            model, subgraph_collection_dict, primary_loss_curve, watermark_loss_curve, mu, sig, p = train_with_watermark(dataset_name, num_node_features, num_classes, train_ratio, val_ratio, test_ratio, subgraph_size=subgraph_size, num_collections=num_collections, seed=seed)\n",
    "            # Right now using default for other values\n",
    "            plt.plot(primary_loss_curve)\n",
    "            plt.plot(watermark_loss_curve)\n",
    "            plt.title(f'{num_collections} collections of {num_subgraphs_per_collection} subgraphs, size {subgraph_size}\\nseed={seed}\\nmu={mu},sig={sig},p={p}')\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc6997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
